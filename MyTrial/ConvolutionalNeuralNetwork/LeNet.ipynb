{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b48003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f237842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2, strides=2),\n",
    "       nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),\n",
    "       nn.MaxPool2D(pool_size=2, strides=2),\n",
    "       nn.Dense(120, activation='sigmoid'),\n",
    "       nn.Dense(84, activation='sigmoid'),\n",
    "       nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5ffd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 output shape:\t (1, 6, 24, 24)\n",
      "pool0 output shape:\t (1, 6, 12, 12)\n",
      "conv1 output shape:\t (1, 16, 8, 8)\n",
      "pool1 output shape:\t (1, 16, 4, 4)\n",
      "dense0 output shape:\t (1, 120)\n",
      "dense1 output shape:\t (1, 84)\n",
      "dense2 output shape:\t (1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\gluon\\lib\\site-packages\\mxnet\\gluon\\parameter.py:862: UserWarning: Parameter 'conv0_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda\\envs\\gluon\\lib\\site-packages\\mxnet\\gluon\\parameter.py:862: UserWarning: Parameter 'conv1_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda\\envs\\gluon\\lib\\site-packages\\mxnet\\gluon\\parameter.py:862: UserWarning: Parameter 'dense0_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda\\envs\\gluon\\lib\\site-packages\\mxnet\\gluon\\parameter.py:862: UserWarning: Parameter 'dense1_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "D:\\Anaconda\\envs\\gluon\\lib\\site-packages\\mxnet\\gluon\\parameter.py:862: UserWarning: Parameter 'dense2_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 28, 28))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d42593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "def try_gpu():\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.zeros((1,), ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "ctx = try_gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d9c041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, ctx):\n",
    "    acc_sum, n = nd.array([0], ctx=ctx), 0\n",
    "    for X, y in data_iter:\n",
    "        # 如果ctx代表gpu及相应显存，则将数据复制到显存上\n",
    "        X, y = X.as_in_context(ctx), y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.size\n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1299fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs):\n",
    "    print('training on', ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec' \n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1742a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 2.3195, train acc 0.101, test acc 0.100, time 6.0 sec\n",
      "epoch 2, loss 1.5665, train acc 0.390, test acc 0.610, time 5.8 sec\n",
      "epoch 3, loss 0.8967, train acc 0.648, test acc 0.713, time 5.8 sec\n",
      "epoch 4, loss 0.7202, train acc 0.716, test acc 0.743, time 5.7 sec\n",
      "epoch 5, loss 0.6409, train acc 0.745, test acc 0.763, time 5.8 sec\n",
      "epoch 6, loss 0.5889, train acc 0.766, test acc 0.789, time 5.7 sec\n",
      "epoch 7, loss 0.5382, train acc 0.787, test acc 0.806, time 5.7 sec\n",
      "epoch 8, loss 0.5097, train acc 0.801, test acc 0.822, time 5.8 sec\n",
      "epoch 9, loss 0.4815, train acc 0.814, test acc 0.836, time 5.7 sec\n",
      "epoch 10, loss 0.4615, train acc 0.824, test acc 0.834, time 5.8 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.9, 10\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':lr})\n",
    "train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66914354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
